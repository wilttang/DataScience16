{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning from the Ground Up\n",
    "\n",
    "Today we will be creating a machine learning algorithm without relying on anything more than code we write ourselves and some basic calculus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-Variable Linear Least Squares\n",
    "\n",
    "In the reading you saw the linear least squares approach to modeling the relationship between two variables.  In the reading, we saw the following model.\n",
    "\n",
    "$y = wx + b + \\epsilon$\n",
    "\n",
    "Where $w$ is the slope of a line describing the relationship between x and y, $b$ is the y-intercept, and $\\epsilon$ is an error term.  To make things even easier, we are going to consider an even more restrictive relationship between x and y.  Specifically, we will assume that the line describing the relationship between x and y goes through the origin (which implies $b = 0$).\n",
    "\n",
    "$y = wx + \\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a line to data\n",
    "\n",
    "Suppose we are given a collection of independent variables $x_1, \\ldots, x_n$ with each $x_i \\in \\mathbb{R}$ and corresponding dependent variables $y_1, \\ldots, y_n$ with $y_i \\in \\mathbb{R}$.  You can think of the x's as height and y's as weight if that helps for concreteness.  We can write an error function that quantifies the sum of squared residuals (remember from the reading?).\n",
    "\n",
    "$e(w) = \\sum_{i=1}^n \\left (wx_i - y_i\\right)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example dataset\n",
    "\n",
    "Next, we will generate some data and examine the geometry of the function $e$.  In order to generate some data, we will use a built-in function scikit learn for generating synthetic datasets.  Synthetic datasets are ones where the relationship between inputs and outputs is known.  These datasets can be good for debugging machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.datasets import make_regression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x, y, coef = make_regression(n_samples=100, n_features=1, noise=10, coef=True)\n",
    "print \"True slope\", coef\n",
    "plt.plot(x,y,'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, plot $e(w)$.  What shape does the function have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def error(w, x, y):\n",
    "    \"\"\" Calculate the sum of squared residuals.  The data points\n",
    "        x and y that are implicit inputs to the our function e\n",
    "        passed as additional inputs to the Python function error\n",
    "    \"\"\"\n",
    "    # YOUR IMPLEMENTATION HERE\n",
    "    return 0.0\n",
    "    \n",
    "ws = np.arange(0,100,1)\n",
    "plt.plot(ws, [error(w,x,y) for w in ws])\n",
    "plt.xlabel(\"w (slope)\")\n",
    "plt.ylabel(\"Sum of squared residuals\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "As discussed in ThinkStats Chapter 10, if we want to have a good model relating x and y, we should choose the slope, $w$, that minimizes the function $e(w)$.\n",
    "\n",
    "In order to find the minimum of $e(w)$ we will use gradient descent.  The basic idea, is to calculate the slope of the function $e(w)$ at some initial guess $w_0$.  We then adjust our guess $w_0$ by moving it in the opposite direction of the slope of the function using this formula.\n",
    "\n",
    "$w_1 = w_0 - \\alpha \\frac{d}{dw}e(w_0)$\n",
    "\n",
    "Where $\\alpha$ is a positive constant that specifies the step size.  To build intuition, determine how the second guess at the slope $w_1$ would change depending on the slope of $e$ evaluated at $w_0$.\n",
    "\n",
    "This procedure can be iterated to construct a series of guesses of the the value of $w$.\n",
    "\n",
    "$w_t = w_{t-1} - \\alpha \\frac{d}{dw}e(w_{t-1})$\n",
    "\n",
    "Next, create a function called `error_grad` that computes the derivative of `error` with respect to $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def error_grad(w, x, y):\n",
    "    \"\"\" Computes the derivative of the error with respect to\n",
    "        the slope w\"\"\"\n",
    "    # YOUR IMPLEMENTATION HERE\n",
    "    return 0.0\n",
    "\n",
    "w_0 = 2\n",
    "estimate = error_grad(w_0, x, y)\n",
    "estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check\n",
    "\n",
    "Use the concept of [numerical differentiation](https://en.wikipedia.org/wiki/Numerical_differentiation) to make sure that your function `error_grad` is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "computed = 0.0     # YOUR IMPLEMENTATION HERE\n",
    "estimate - computed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of Gradient Descent\n",
    "\n",
    "Now that we are reasonably confident in our gradient computation, we can proceed to implement gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_descent(w, x, y, alpha, iters):\n",
    "    \"\"\" Perform `iters` iterations of gradient descent\n",
    "        with the initial guess w, independent variables, x,\n",
    "        and dependent variables, y, and step size, alpha. \"\"\"\n",
    "    errors = np.zeros((iters,1))\n",
    "    # WE WILL WRITE THIS TOGETHER\n",
    "    return w, errors\n",
    "\n",
    "w_f, errors = gradient_descent(w_0, x, y, .001, 100)\n",
    "print w_f\n",
    "plt.plot(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This basic approach to gradient descent is somewhat finicky.  Fortunately, we can do better by dynamically adjusting alpha depending on whether the error is increasing or decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_descent_adaptive(w, x, y, alpha, iters):\n",
    "    \"\"\" Perform `iters` iterations of gradient descent\n",
    "        with the initial guess w, independent variables, x,\n",
    "        and dependent variables, y, and step size, alpha.\n",
    "        \n",
    "        The step size will automatically be adapted for good\n",
    "        performance. \"\"\"\n",
    "    errors = np.zeros((iters,1))\n",
    "\n",
    "    # WE WILL WRITE THIS TOGETHER\n",
    "            \n",
    "    return w, errors\n",
    "\n",
    "w_f, errors = gradient_descent_adaptive(w_0, x, y, .01, 100)\n",
    "print w_f\n",
    "plt.plot(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Linear Least Squares\n",
    "\n",
    "All of the theory we just developed can be applied to the case where we have multiple independent variables that we'd like to use to model a single dependent variable.  In this case, we assume that the datapoints are now given as $x_1, \\ldots, x_n$ with $x_i \\in \\mathbb{R}^d$.  As before we are also given corresponding targets $y_1, \\ldots, y_n$ with $y_i \\in \\mathbb{R}$.  Now, the relationship between $y_i$ and $x_i$ is modeled as:\n",
    "\n",
    "$y_i = \\sum_{j=1}^d w_jx_i + \\epsilon$\n",
    "\n",
    "### Computing the Best Weights\n",
    "\n",
    "In order to compute the best weights we have to define what we mean by best.  We will use the exact same notion of \"error\" that we used for the single variable case, that is the sum of squared errors.  To make this more concrete, let's load a dataset with multiple variables that we can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from load_smiles import load_smiles\n",
    "\n",
    "def show_smiles(images, targets):\n",
    "    \"\"\" Adapted from Jake Vanderplas' scikit learn tutorials. \"\"\"\n",
    "    fig, axes = plt.subplots(6, 6, figsize=(24, 24))\n",
    "    fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(images[i,:].reshape((24,24)).T, cmap='gray')\n",
    "        ax.text(0.05, 0.05, str(targets[i]),\n",
    "                transform=ax.transAxes,\n",
    "                color='green' if (targets[i]) else 'red')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "data = load_smiles()\n",
    "X, y = data.data, data.target\n",
    "show_smiles(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Error Function\n",
    "\n",
    "Write some code to calculate the error for a set of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def error_multi(w, X, y):\n",
    "    \"\"\" Calculate the sum of squares of the residuals\n",
    "        for a multivariate regression model.  w contains\n",
    "        the model parameters (n_features x 1).  X contains\n",
    "        a matrix of predictor variables (n_samples x n_features).\n",
    "        y contains the dependent variables (n_samples x 1)\"\"\"\n",
    "    # YOUR IMPLEMENTATION HERE\n",
    "    return 0.0\n",
    "    \n",
    "\n",
    "w = np.zeros((576,1))\n",
    "error_multi(w, data.data, data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Gradient\n",
    "\n",
    "The gradient of a function is a vector of all of the function's partial derivatives.\n",
    "\n",
    "$\\nabla f(x) = \\begin{bmatrix} \\frac{\\partial}{\\partial x_1}f(x) \\\\ \\frac{\\partial}{\\partial x_2}f(x) \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial x_d}f(x) \\end{bmatrix}$\n",
    "\n",
    "Next, we will write a function to compute the $i$th partial derivative of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def partial_error_multi(w, X, y, j):\n",
    "    \"\"\" Compute the jth partial derivative of the error function \"\"\"\n",
    "    # YOUR IMPLEMENTATION\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the single variable case, perform a sanity check on your computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Computed partial 0\", 0.0 # YOUR IMPLEMENTATION HERE\n",
    "print \"Analytical partial 0\", partial_error_multi(w, X, y, 0)\n",
    "\n",
    "print \"Computed partial 1\", 0.0 # YOUR IMPLEMENTATION HERE\n",
    "print \"Analytical partial 1\", partial_error_multi(w, X, y, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed things up, we make the observation that the predictions are independent of $j$, therefore we are wasting a lot of time computing them for reach $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grad_error_multi(w, X, y):\n",
    "    grad = np.zeros(w.shape)\n",
    "    res = X.dot(w) - y\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        grad += 2*res[i]*X[i,np.newaxis].T\n",
    "    return grad\n",
    "\n",
    "print grad_error_multi(w, X, y)[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_descent_multi(w, x, y, alpha, iters):\n",
    "    errors = np.zeros((iters,1))\n",
    "    last_error = error_multi(w, x, y)\n",
    "    all_ws = []\n",
    "\n",
    "    for i in range(iters):\n",
    "        all_ws.append(w)\n",
    "        grad = grad_error_multi(w, x, y)\n",
    "        w_proposed = w - alpha*grad\n",
    "        error_proposed = error_multi(w_proposed, x, y)\n",
    "        if error_proposed < last_error:\n",
    "            last_error = error_proposed\n",
    "            w = w_proposed\n",
    "            alpha *= 1.1\n",
    "        else:\n",
    "            alpha *= 0.2\n",
    "        if i % 100 == 0:\n",
    "            print \"iter\", i, \"error\", last_error\n",
    "        errors[i] = last_error\n",
    "    return w, errors, all_ws\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.2)\n",
    "\n",
    "w_f, errors, all_ws = gradient_descent_multi(w, X_train, y_train, 10**-7, 200)\n",
    "plt.plot(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.mean((X_test.dot(w_f) > 0.5) == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.html import widgets\n",
    "\n",
    "def image_display(i):\n",
    "    plt.imshow(all_ws[i].reshape((24,24)).T, cmap='gray')\n",
    "\n",
    "step_slider = widgets.IntSlider(min=0, max=len(all_ws)-1, value=0)\n",
    "widgets.interact(image_display, i=step_slider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
